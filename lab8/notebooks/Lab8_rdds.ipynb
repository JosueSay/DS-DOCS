{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f389ecaa",
   "metadata": {},
   "source": [
    "# Lab 8 RDDS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff249cb",
   "metadata": {},
   "source": [
    "- [Repositorio](https://github.com/JosueSay/DS-DOCS/tree/main/lab8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428bf8fd",
   "metadata": {},
   "source": [
    "## Librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5fda0c44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download(\"stopwords\", quiet=True) # Descargar stopwords si no están descargadas\n",
    "# docker compose up -d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f11710a",
   "metadata": {},
   "source": [
    "## Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "44edca60-5ba2-40bc-a474-7a4d81505d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"MiApp\").master(\"local[*]\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "working_dir = \"/opt/app/working_dir/\"\n",
    "rdd = sc.textFile(working_dir + \"constitution.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ed33c1",
   "metadata": {},
   "source": [
    "## Carga de RDD y exploración"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a6e3d8b4-dc9d-45ad-844c-67b81de059ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Exploración inicial ===\n",
      "Línea 1: We the People of the United States, in Order to form a more perfect \n",
      "Línea 2: Union, establish Justice, insure domestic Tranquility, provide for the \n",
      "Línea 3: common defence, promote the general Welfare, and secure the Blessings of \n",
      "\n",
      "Total de líneas en el documento: 649\n"
     ]
    }
   ],
   "source": [
    "primeras_lineas = rdd.take(3)\n",
    "num_lineas = rdd.count()\n",
    "\n",
    "print(\"=== Exploración inicial ===\")\n",
    "for i, l in enumerate(primeras_lineas, 1):\n",
    "    print(f\"Línea {i}: {l}\")\n",
    "print(f\"\\nTotal de líneas en el documento: {num_lineas:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb24fc81",
   "metadata": {},
   "source": [
    "## Word Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "616dd1f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ejemplo con map (3 elementos):\n",
      "[['We', 'the', 'People', 'of', 'the', 'United', 'States,', 'in', 'Order', 'to', 'form', 'a', 'more', 'perfect', ''], ['Union,', 'establish', 'Justice,', 'insure', 'domestic', 'Tranquility,', 'provide', 'for', 'the', ''], ['common', 'defence,', 'promote', 'the', 'general', 'Welfare,', 'and', 'secure', 'the', 'Blessings', 'of', '']]\n",
      "\n",
      "Total de 'tokens' limpios: 6,701\n",
      "Palabra más larga: 'constitutionally'\n"
     ]
    }
   ],
   "source": [
    "splitted_lines = rdd.map(lambda line: line.split(' '))\n",
    "print(\"\\nEjemplo con map (3 elementos):\")\n",
    "print(splitted_lines.take(3))\n",
    "\n",
    "# Pipeline de limpieza + normalización\n",
    "words_rdd = (\n",
    "    rdd.flatMap(lambda line: line.strip().split(' '))\n",
    "       .map(lambda w: w.strip())\n",
    "       .filter(lambda w: w != '' and w.isalnum())\n",
    "       .map(lambda w: w.lower())\n",
    ")\n",
    "\n",
    "num_palabras = words_rdd.count()\n",
    "print(f\"\\nTotal de 'tokens' limpios: {num_palabras:,}\")\n",
    "\n",
    "# Palabra más larga (reduce)\n",
    "mas_larga = words_rdd.reduce(lambda a, b: a if len(a) > len(b) else b)\n",
    "print(f\"Palabra más larga: '{mas_larga}'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee186cd",
   "metadata": {},
   "source": [
    "## Conteos y Top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "37365542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 5 (incluyendo stopwords):\n",
      "1. the -> 726\n",
      "2. of -> 493\n",
      "3. shall -> 293\n",
      "4. and -> 262\n",
      "5. to -> 201\n"
     ]
    }
   ],
   "source": [
    "keyval_rdd = words_rdd.map(lambda w: (w, 1))\n",
    "wordcount = keyval_rdd.reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "top5 = (wordcount\n",
    "        .map(lambda kv: (kv[1], kv[0]))\n",
    "        .sortByKey(ascending=False)\n",
    "        .take(5))\n",
    "\n",
    "print(\"\\nTop 5 (incluyendo stopwords):\")\n",
    "for rank, (freq, word) in enumerate(top5, 1):\n",
    "    print(f\"{rank}. {word} -> {freq}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09543ca",
   "metadata": {},
   "source": [
    "## Top-N sin stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5d5d5855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 sin stopwords:\n",
      "1. united -> 85\n",
      "2. president -> 72\n",
      "3. may -> 42\n",
      "4. congress -> 39\n",
      "5. amendment -> 34\n"
     ]
    }
   ],
   "source": [
    "# Stopwords base + términos frecuentes del dominio legal/constitucional\n",
    "sw = set(stopwords.words(\"english\"))\n",
    "domain_sw = {\"shall\", \"section\", \"sections\", \"article\", \"articles\", \"state\", \"states\"}\n",
    "stopwords_all = sw.union(domain_sw)\n",
    "\n",
    "# Filtrar y obtener top 5 sin stopwords\n",
    "top5_no_stop = (\n",
    "    wordcount\n",
    "    .filter(lambda kv: kv[0] not in stopwords_all)\n",
    "    .map(lambda kv: (kv[1], kv[0]))\n",
    "    .sortByKey(ascending=False)\n",
    "    .take(5)\n",
    ")\n",
    "\n",
    "print(\"Top 5 sin stopwords:\")\n",
    "for i, (freq, word) in enumerate(top5_no_stop, 1):\n",
    "    print(f\"{i}. {word} -> {freq}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c35c87fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9700c2d0",
   "metadata": {},
   "source": [
    "## Discusión y Conclusiones\n",
    "\n",
    "Se observó que el documento contenía **649 líneas**.\n",
    "\n",
    "Al aplicar transformaciones con `map` y `flatMap`, se pudo observar la diferencia entre ambas operaciones: `map` devolvió listas anidadas, por lo que un `splitted_lines.count()` solo devolvería el número de **líneas** y no el total de palabras, lo cual no cumplía con nuestro propósito de análisis léxico. En cambio, `flatMap` permitió generar un flujo continuo de palabras, haciendo posible un conteo real de tokens en el texto.  \n",
    "\n",
    "Posteriormente, se aplicó un proceso de limpieza eliminando cadenas vacías, normalizando todas las palabras a minúsculas y utilizando el método `.isalnum()` para asegurar que el RDD `words_rdd` contuviera únicamente palabras formadas por caracteres alfanuméricos. Gracias a esto se contabilizaron **6,701 palabras**.  \n",
    "\n",
    "Se aplicó la operación `reduce` para identificar la palabra más larga, encontrándose **\"constitutionally\"** como resultado.  \n",
    "\n",
    "Se construyó un RDD de pares clave-valor y se calcularon las frecuencias. Al ordenar los resultados se obtuvo el **Top 5 incluyendo stopwords**, conformado por:  \n",
    "1. *the* (726)  \n",
    "2. *of* (493)  \n",
    "3. *shall* (293)  \n",
    "4. *and* (262)  \n",
    "5. *to* (201)  \n",
    "\n",
    "**Pregunta: Muestre las 5 palabras más repetidas excluyendo las stopwords.**  \n",
    "\n",
    "Para dar respuesta, se aplicó un filtro adicional con las stopwords provistas por NLTK, ampliadas con términos del dominio constitucional como *shall*, *section* o *article*. Tras este proceso, el **Top 5 sin stopwords** fue:  \n",
    "\n",
    "1. *united* (85)  \n",
    "2. *president* (72)  \n",
    "3. *may* (42)  \n",
    "4. *congress* (39)  \n",
    "5. *amendment* (34)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
